{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452e1541",
   "metadata": {},
   "source": [
    "# 权限配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c8330a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::064542430558:role/service-role/AmazonSageMaker-ExecutionRole-20200803T154438\n",
      "sagemaker bucket: sagemaker-us-west-2-064542430558\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcebdc8",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82806b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-064542430558/lhr-data/shulex'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset used\n",
    "dataset_name = 'shulex'\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'lhr-data/shulex'\n",
    "WORK_DIRECTORY = './data/'\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=s3_prefix)\n",
    "data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e83404",
   "metadata": {},
   "source": [
    "# 超参数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "796d7849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters which are passed to the training job\n",
    "hyperparameters={'reference_column':'ref',\n",
    "                 'hypothesis_column':'hyp',\n",
    "                 'train_file':'/opt/ml/input/data/train/shulexv2_train.csv',\n",
    "                 'validation_file':'/opt/ml/input/data/test/shulexv2_dev.csv',\n",
    "                 'output_dir':'/opt/ml/model',\n",
    "                 'do_train':True,\n",
    "                 'do_eval':True,\n",
    "                 'max_source_length': 128,\n",
    "                 'max_target_length': 64,\n",
    "                 'model_name_or_path': 't5-base',\n",
    "                 'learning_rate': 3e-4,\n",
    "                 'num_train_epochs': 20,\n",
    "                 'per_device_train_batch_size': 16,#16\n",
    "                 'gradient_accumulation_steps':2, \n",
    "                 'save_strategy':'epoch',\n",
    "                 'evaluation_strategy':'epoch',\n",
    "                 'save_total_limit':1,\n",
    "                 }\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='run_paraphrase.py',\n",
    "        source_dir='./scripts',\n",
    "        instance_type='ml.p3.2xlarge',#'ml.p3dn.24xlarge'\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        max_run=24*60*60,\n",
    "        transformers_version='4.6',\n",
    "        pytorch_version='1.7',\n",
    "        py_version='py36',\n",
    "        volume_size=128,\n",
    "        hyperparameters = hyperparameters,\n",
    "#         distribution=distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f902e",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11004212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-26 15:07:35 Starting - Starting the training job...\n",
      "2022-09-26 15:08:05 Starting - Preparing the instances for trainingProfilerReport-1664204855: InProgress\n",
      ".........\n",
      "2022-09-26 15:09:21 Downloading - Downloading input data...\n",
      "2022-09-26 15:10:01 Training - Downloading the training image.....................\n",
      "2022-09-26 15:13:24 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-09-26 15:13:26,832 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-09-26 15:13:26,864 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-09-26 15:13:26,871 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-09-26 15:13:27,361 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (3.17.1)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\u001b[0m\n",
      "\u001b[34m  Downloading rouge_score-0.1.1.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting py7zr\n",
      "  Downloading py7zr-0.19.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers==4.6.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (4.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 7)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 7)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 7)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 7)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 7)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 7)) (0.10.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 7)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 7)) (0.0.45)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 7)) (2021.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub==0.0.8 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 7)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 7)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r requirements.txt (line 7)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r requirements.txt (line 7)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r requirements.txt (line 7)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r requirements.txt (line 7)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.9.13-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (756 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 5)) (7.1.2)\u001b[0m\n",
      "\u001b[34mCollecting pybcj>=0.6.0\n",
      "  Downloading pybcj-1.0.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\u001b[0m\n",
      "\u001b[34mCollecting pycryptodomex>=3.6.6\n",
      "  Downloading pycryptodomex-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting pyzstd>=0.14.4\n",
      "  Downloading pyzstd-0.15.3-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (379 kB)\u001b[0m\n",
      "\u001b[34mCollecting texttable\n",
      "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting brotli>=1.0.9\n",
      "  Downloading Brotli-1.0.9-cp36-cp36m-manylinux1_x86_64.whl (357 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyppmd<0.19.0,>=0.18.1\n",
      "  Downloading pyppmd-0.18.3-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\u001b[0m\n",
      "\u001b[34mCollecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (from py7zr->-r requirements.txt (line 6)) (5.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1->-r requirements.txt (line 7)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.1->-r requirements.txt (line 7)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.1-py3-none-any.whl size=24939 sha256=9f7c0422ae68fbf999a145a41e4631ebea0a030e4bf47e58e89c2976cdf1ba01\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/fc/6e/a34132fbbbd4a2822f51a26966c2fad1d9d8106374735367e1\u001b[0m\n",
      "\u001b[34mSuccessfully built rouge-score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, texttable, pyzstd, pyppmd, pycryptodomex, pybcj, nltk, multivolumefile, brotli, absl-py, rouge-score, py7zr\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.2.0 brotli-1.0.9 multivolumefile-0.2.3 nltk-3.6.7 py7zr-0.19.0 pybcj-1.0.1 pycryptodomex-3.15.0 pyppmd-0.18.3 pyzstd-0.15.3 regex-2022.9.13 rouge-score-0.1.1 texttable-1.6.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-09-26 15:13:37,029 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"do_eval\": true,\n",
      "        \"do_train\": true,\n",
      "        \"evaluation_strategy\": \"epoch\",\n",
      "        \"gradient_accumulation_steps\": 2,\n",
      "        \"hypothesis_column\": \"hyp\",\n",
      "        \"learning_rate\": 0.0003,\n",
      "        \"max_source_length\": 128,\n",
      "        \"max_target_length\": 64,\n",
      "        \"model_name_or_path\": \"t5-base\",\n",
      "        \"num_train_epochs\": 20,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_train_batch_size\": 16,\n",
      "        \"reference_column\": \"ref\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"save_total_limit\": 1,\n",
      "        \"train_file\": \"/opt/ml/input/data/train/shulexv2_train.csv\",\n",
      "        \"validation_file\": \"/opt/ml/input/data/test/shulexv2_dev.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-09-26-15-07-35-034\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-064542430558/huggingface-pytorch-training-2022-09-26-15-07-35-034/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_paraphrase\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_paraphrase.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_train\":true,\"evaluation_strategy\":\"epoch\",\"gradient_accumulation_steps\":2,\"hypothesis_column\":\"hyp\",\"learning_rate\":0.0003,\"max_source_length\":128,\"max_target_length\":64,\"model_name_or_path\":\"t5-base\",\"num_train_epochs\":20,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":16,\"reference_column\":\"ref\",\"save_strategy\":\"epoch\",\"save_total_limit\":1,\"train_file\":\"/opt/ml/input/data/train/shulexv2_train.csv\",\"validation_file\":\"/opt/ml/input/data/test/shulexv2_dev.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_paraphrase.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_paraphrase\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-064542430558/huggingface-pytorch-training-2022-09-26-15-07-35-034/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"evaluation_strategy\":\"epoch\",\"gradient_accumulation_steps\":2,\"hypothesis_column\":\"hyp\",\"learning_rate\":0.0003,\"max_source_length\":128,\"max_target_length\":64,\"model_name_or_path\":\"t5-base\",\"num_train_epochs\":20,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":16,\"reference_column\":\"ref\",\"save_strategy\":\"epoch\",\"save_total_limit\":1,\"train_file\":\"/opt/ml/input/data/train/shulexv2_train.csv\",\"validation_file\":\"/opt/ml/input/data/test/shulexv2_dev.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-09-26-15-07-35-034\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-064542430558/huggingface-pytorch-training-2022-09-26-15-07-35-034/source/sourcedir.tar.gz\",\"module_name\":\"run_paraphrase\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_paraphrase.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--evaluation_strategy\",\"epoch\",\"--gradient_accumulation_steps\",\"2\",\"--hypothesis_column\",\"hyp\",\"--learning_rate\",\"0.0003\",\"--max_source_length\",\"128\",\"--max_target_length\",\"64\",\"--model_name_or_path\",\"t5-base\",\"--num_train_epochs\",\"20\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_train_batch_size\",\"16\",\"--reference_column\",\"ref\",\"--save_strategy\",\"epoch\",\"--save_total_limit\",\"1\",\"--train_file\",\"/opt/ml/input/data/train/shulexv2_train.csv\",\"--validation_file\",\"/opt/ml/input/data/test/shulexv2_dev.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_HYPOTHESIS_COLUMN=hyp\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0003\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SOURCE_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TARGET_LENGTH=64\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=t5-base\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=20\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_REFERENCE_COLUMN=ref\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=1\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/shulexv2_train.csv\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/test/shulexv2_dev.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_paraphrase.py --do_eval True --do_train True --evaluation_strategy epoch --gradient_accumulation_steps 2 --hypothesis_column hyp --learning_rate 0.0003 --max_source_length 128 --max_target_length 64 --model_name_or_path t5-base --num_train_epochs 20 --output_dir /opt/ml/model --per_device_train_batch_size 16 --reference_column ref --save_strategy epoch --save_total_limit 1 --train_file /opt/ml/input/data/train/shulexv2_train.csv --validation_file /opt/ml/input/data/test/shulexv2_dev.csv\u001b[0m\n",
      "\u001b[34m09/26/2022 15:13:43 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m09/26/2022 15:13:43 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/opt/ml/model', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=2, eval_accumulation_steps=None, learning_rate=0.0003, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=20.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Sep26_15-13-42_algo-1', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.EPOCH: 'epoch'>, save_steps=500, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/opt/ml/model', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=False)\u001b[0m\n",
      "\u001b[34m09/26/2022 15:13:43 - WARNING - datasets.builder -   Using custom data configuration default-8d809a9031961118\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-8d809a9031961118/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-8d809a9031961118/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mhttps://huggingface.co/t5-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpt4d6mauc\u001b[0m\n",
      "\u001b[34mstoring https://huggingface.co/t5-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\u001b[0m\n",
      "\u001b[34mcreating metadata file for /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\u001b[0m\n",
      "\u001b[34mloading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\u001b[0m\n",
      "\u001b[34mModel config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mloading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\u001b[0m\n",
      "\u001b[34mModel config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttps://huggingface.co/t5-base/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpieamzr3m\u001b[0m\n",
      "\u001b[34mstoring https://huggingface.co/t5-base/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\u001b[0m\n",
      "\u001b[34mcreating metadata file for /root/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\u001b[0m\n",
      "\u001b[34mhttps://huggingface.co/t5-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprqfy_bxh\u001b[0m\n",
      "\u001b[34mstoring https://huggingface.co/t5-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\u001b[0m\n",
      "\u001b[34mcreating metadata file for /root/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\u001b[0m\n",
      "\u001b[34mloading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\u001b[0m\n",
      "\u001b[34mloading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\u001b[0m\n",
      "\u001b[34mloading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34mloading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34mloading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34mhttps://huggingface.co/t5-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmg4jctj0\u001b[0m\n",
      "\u001b[34mstoring https://huggingface.co/t5-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\u001b[0m\n",
      "\u001b[34mcreating metadata file for /root/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\u001b[0m\n",
      "\u001b[34mloading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\u001b[0m\n",
      "\u001b[34mAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\u001b[0m\n",
      "\u001b[34mAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 8396\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 5240\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:19.849 algo-1:38 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.008 algo-1:38 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.009 algo-1:38 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.010 algo-1:38 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.011 algo-1:38 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.011 algo-1:38 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.209 algo-1:38 INFO hook.py:591] name:shared.weight count_params:24652800\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.209 algo-1:38 INFO hook.py:591] name:encoder.block.0.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.209 algo-1:38 INFO hook.py:591] name:encoder.block.0.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.209 algo-1:38 INFO hook.py:591] name:encoder.block.0.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.209 algo-1:38 INFO hook.py:591] name:encoder.block.0.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.209 algo-1:38 INFO hook.py:591] name:encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.210 algo-1:38 INFO hook.py:591] name:encoder.block.0.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.210 algo-1:38 INFO hook.py:591] name:encoder.block.0.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.210 algo-1:38 INFO hook.py:591] name:encoder.block.0.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.210 algo-1:38 INFO hook.py:591] name:encoder.block.0.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.210 algo-1:38 INFO hook.py:591] name:encoder.block.1.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.210 algo-1:38 INFO hook.py:591] name:encoder.block.1.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.211 algo-1:38 INFO hook.py:591] name:encoder.block.1.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.211 algo-1:38 INFO hook.py:591] name:encoder.block.1.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.211 algo-1:38 INFO hook.py:591] name:encoder.block.1.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.211 algo-1:38 INFO hook.py:591] name:encoder.block.1.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.211 algo-1:38 INFO hook.py:591] name:encoder.block.1.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.212 algo-1:38 INFO hook.py:591] name:encoder.block.1.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.212 algo-1:38 INFO hook.py:591] name:encoder.block.2.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.212 algo-1:38 INFO hook.py:591] name:encoder.block.2.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.212 algo-1:38 INFO hook.py:591] name:encoder.block.2.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.212 algo-1:38 INFO hook.py:591] name:encoder.block.2.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.212 algo-1:38 INFO hook.py:591] name:encoder.block.2.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.212 algo-1:38 INFO hook.py:591] name:encoder.block.2.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.213 algo-1:38 INFO hook.py:591] name:encoder.block.2.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.213 algo-1:38 INFO hook.py:591] name:encoder.block.2.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.213 algo-1:38 INFO hook.py:591] name:encoder.block.3.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.213 algo-1:38 INFO hook.py:591] name:encoder.block.3.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.213 algo-1:38 INFO hook.py:591] name:encoder.block.3.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.213 algo-1:38 INFO hook.py:591] name:encoder.block.3.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.213 algo-1:38 INFO hook.py:591] name:encoder.block.3.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.213 algo-1:38 INFO hook.py:591] name:encoder.block.3.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.214 algo-1:38 INFO hook.py:591] name:encoder.block.3.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.214 algo-1:38 INFO hook.py:591] name:encoder.block.3.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.214 algo-1:38 INFO hook.py:591] name:encoder.block.4.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.214 algo-1:38 INFO hook.py:591] name:encoder.block.4.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.214 algo-1:38 INFO hook.py:591] name:encoder.block.4.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.214 algo-1:38 INFO hook.py:591] name:encoder.block.4.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.214 algo-1:38 INFO hook.py:591] name:encoder.block.4.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.215 algo-1:38 INFO hook.py:591] name:encoder.block.4.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.215 algo-1:38 INFO hook.py:591] name:encoder.block.4.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.215 algo-1:38 INFO hook.py:591] name:encoder.block.4.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.215 algo-1:38 INFO hook.py:591] name:encoder.block.5.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.215 algo-1:38 INFO hook.py:591] name:encoder.block.5.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.215 algo-1:38 INFO hook.py:591] name:encoder.block.5.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.215 algo-1:38 INFO hook.py:591] name:encoder.block.5.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.215 algo-1:38 INFO hook.py:591] name:encoder.block.5.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.216 algo-1:38 INFO hook.py:591] name:encoder.block.5.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.216 algo-1:38 INFO hook.py:591] name:encoder.block.5.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.216 algo-1:38 INFO hook.py:591] name:encoder.block.5.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.216 algo-1:38 INFO hook.py:591] name:encoder.block.6.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.216 algo-1:38 INFO hook.py:591] name:encoder.block.6.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.216 algo-1:38 INFO hook.py:591] name:encoder.block.6.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.217 algo-1:38 INFO hook.py:591] name:encoder.block.6.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.217 algo-1:38 INFO hook.py:591] name:encoder.block.6.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.217 algo-1:38 INFO hook.py:591] name:encoder.block.6.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.217 algo-1:38 INFO hook.py:591] name:encoder.block.6.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.217 algo-1:38 INFO hook.py:591] name:encoder.block.6.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.217 algo-1:38 INFO hook.py:591] name:encoder.block.7.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.218 algo-1:38 INFO hook.py:591] name:encoder.block.7.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.218 algo-1:38 INFO hook.py:591] name:encoder.block.7.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.218 algo-1:38 INFO hook.py:591] name:encoder.block.7.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.218 algo-1:38 INFO hook.py:591] name:encoder.block.7.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.218 algo-1:38 INFO hook.py:591] name:encoder.block.7.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.218 algo-1:38 INFO hook.py:591] name:encoder.block.7.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.219 algo-1:38 INFO hook.py:591] name:encoder.block.7.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.219 algo-1:38 INFO hook.py:591] name:encoder.block.8.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.219 algo-1:38 INFO hook.py:591] name:encoder.block.8.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.219 algo-1:38 INFO hook.py:591] name:encoder.block.8.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.219 algo-1:38 INFO hook.py:591] name:encoder.block.8.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.219 algo-1:38 INFO hook.py:591] name:encoder.block.8.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.220 algo-1:38 INFO hook.py:591] name:encoder.block.8.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.220 algo-1:38 INFO hook.py:591] name:encoder.block.8.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.220 algo-1:38 INFO hook.py:591] name:encoder.block.8.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.220 algo-1:38 INFO hook.py:591] name:encoder.block.9.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.221 algo-1:38 INFO hook.py:591] name:encoder.block.9.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.221 algo-1:38 INFO hook.py:591] name:encoder.block.9.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.221 algo-1:38 INFO hook.py:591] name:encoder.block.9.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.221 algo-1:38 INFO hook.py:591] name:encoder.block.9.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.221 algo-1:38 INFO hook.py:591] name:encoder.block.9.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.221 algo-1:38 INFO hook.py:591] name:encoder.block.9.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.222 algo-1:38 INFO hook.py:591] name:encoder.block.9.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.222 algo-1:38 INFO hook.py:591] name:encoder.block.10.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.222 algo-1:38 INFO hook.py:591] name:encoder.block.10.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.222 algo-1:38 INFO hook.py:591] name:encoder.block.10.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.222 algo-1:38 INFO hook.py:591] name:encoder.block.10.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.222 algo-1:38 INFO hook.py:591] name:encoder.block.10.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.223 algo-1:38 INFO hook.py:591] name:encoder.block.10.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.223 algo-1:38 INFO hook.py:591] name:encoder.block.10.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.223 algo-1:38 INFO hook.py:591] name:encoder.block.10.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.223 algo-1:38 INFO hook.py:591] name:encoder.block.11.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.223 algo-1:38 INFO hook.py:591] name:encoder.block.11.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.223 algo-1:38 INFO hook.py:591] name:encoder.block.11.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.223 algo-1:38 INFO hook.py:591] name:encoder.block.11.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.224 algo-1:38 INFO hook.py:591] name:encoder.block.11.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.224 algo-1:38 INFO hook.py:591] name:encoder.block.11.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.224 algo-1:38 INFO hook.py:591] name:encoder.block.11.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.224 algo-1:38 INFO hook.py:591] name:encoder.block.11.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.224 algo-1:38 INFO hook.py:591] name:encoder.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.224 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.224 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.224 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.225 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.225 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.225 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.225 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.225 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.225 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.225 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.225 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.226 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.226 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.226 algo-1:38 INFO hook.py:591] name:decoder.block.0.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.226 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.226 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.226 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.226 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.227 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.227 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.227 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.227 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.227 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.227 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.227 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.227 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.228 algo-1:38 INFO hook.py:591] name:decoder.block.1.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.228 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.228 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.228 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.228 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.228 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.229 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.229 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.229 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.229 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.229 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.230 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.230 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.230 algo-1:38 INFO hook.py:591] name:decoder.block.2.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.230 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.230 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.230 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.231 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.231 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.231 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.231 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.232 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.232 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.232 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.232 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.232 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.233 algo-1:38 INFO hook.py:591] name:decoder.block.3.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.233 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.233 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.233 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.234 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.234 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.234 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.234 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.234 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.234 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.235 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.235 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.235 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.235 algo-1:38 INFO hook.py:591] name:decoder.block.4.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.235 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.235 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.235 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.235 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.235 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.236 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.236 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.236 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.236 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.236 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.236 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.236 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.236 algo-1:38 INFO hook.py:591] name:decoder.block.5.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.236 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.237 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.237 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.237 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.237 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.237 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.237 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.237 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.237 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.237 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.238 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.238 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.238 algo-1:38 INFO hook.py:591] name:decoder.block.6.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.238 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.238 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.238 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.238 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.239 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.239 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.239 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.239 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.239 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.239 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.239 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.240 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.240 algo-1:38 INFO hook.py:591] name:decoder.block.7.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.240 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.240 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.240 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.240 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.240 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.241 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.241 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.241 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.241 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.241 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.241 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.241 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.242 algo-1:38 INFO hook.py:591] name:decoder.block.8.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.242 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.242 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.242 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.242 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.242 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.242 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.242 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.243 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.243 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.243 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.243 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.243 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.243 algo-1:38 INFO hook.py:591] name:decoder.block.9.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.243 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.243 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.243 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.10.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.244 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:591] name:decoder.block.11.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:591] name:decoder.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:593] Total Trainable Params: 222882048\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.245 algo-1:38 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-09-26 15:14:20.248 algo-1:38 INFO hook.py:488] Hook is writing from the hook with pid: 38\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.09585607796907425, 'eval_runtime': 42.2527, 'eval_samples_per_second': 49.417, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-262\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-262/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-262/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-262/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-262/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-262/spiece.model\u001b[0m\n",
      "\u001b[34m{'loss': 0.1487, 'learning_rate': 0.0002713740458015267, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.08860315382480621, 'eval_runtime': 42.1767, 'eval_samples_per_second': 49.506, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-524\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-524/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-524/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-524/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-524/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-524/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-262] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.08399296551942825, 'eval_runtime': 40.9907, 'eval_samples_per_second': 50.938, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-786\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-786/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-786/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-786/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-786/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-786/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-524] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.048, 'learning_rate': 0.00024274809160305342, 'epoch': 3.82}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.09028613567352295, 'eval_runtime': 41.5503, 'eval_samples_per_second': 50.252, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-1048\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-1048/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-1048/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-1048/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1048/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-1048/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-786] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.09511911869049072, 'eval_runtime': 41.5665, 'eval_samples_per_second': 50.233, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-1310\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-1310/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-1310/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-1310/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1310/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-1310/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-1048] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.023, 'learning_rate': 0.00021412213740458014, 'epoch': 5.72}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.10817617923021317, 'eval_runtime': 41.3597, 'eval_samples_per_second': 50.484, 'epoch': 6.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-1572\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-1572/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-1572/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-1572/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1572/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-1572/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-1310] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.10966804623603821, 'eval_runtime': 44.1844, 'eval_samples_per_second': 47.257, 'epoch': 7.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-1834\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-1834/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-1834/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-1834/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-1834/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-1834/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-1572] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0121, 'learning_rate': 0.00018549618320610686, 'epoch': 7.63}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.13327090442180634, 'eval_runtime': 41.6608, 'eval_samples_per_second': 50.119, 'epoch': 8.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-2096\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-2096/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-2096/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-2096/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-2096/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-2096/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-1834] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.15467402338981628, 'eval_runtime': 41.3149, 'eval_samples_per_second': 50.539, 'epoch': 9.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-2358\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-2358/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-2358/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-2358/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-2358/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-2358/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-2096] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0075, 'learning_rate': 0.00015687022900763358, 'epoch': 9.54}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1365251988172531, 'eval_runtime': 42.6145, 'eval_samples_per_second': 48.997, 'epoch': 10.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-2620\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-2620/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-2620/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-2620/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-2620/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-2620/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-2358] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.14000099897384644, 'eval_runtime': 41.3528, 'eval_samples_per_second': 50.492, 'epoch': 11.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-2882\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-2882/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-2882/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-2882/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-2882/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-2882/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-2620] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0045, 'learning_rate': 0.00012824427480916028, 'epoch': 11.45}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.14693747460842133, 'eval_runtime': 42.3486, 'eval_samples_per_second': 49.305, 'epoch': 12.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-3144\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-3144/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-3144/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-3144/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-3144/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-3144/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-2882] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.14726337790489197, 'eval_runtime': 41.915, 'eval_samples_per_second': 49.815, 'epoch': 13.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-3406\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-3406/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-3406/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-3406/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-3406/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-3406/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-3144] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0025, 'learning_rate': 9.961832061068701e-05, 'epoch': 13.36}\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 2088\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.15697740018367767, 'eval_runtime': 43.9924, 'eval_samples_per_second': 47.463, 'epoch': 14.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-3668\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-3668/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-3668/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-3668/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-3668/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-3668/spiece.model\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/opt/ml/model/checkpoint-3406] due to args.save_total_limit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(\n",
    "  {'train': data_location+'/shulexv2_train.csv',\n",
    "   'test': data_location+'/shulexv2_dev.csv'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e0c9f",
   "metadata": {},
   "source": [
    "# 模型加载&部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6eda483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "#    env= {'HF_TASK':'text-generation'},\n",
    "   model_data=\"s3://sagemaker-us-west-2-064542430558/huggingface-pytorch-training-2022-09-26-15-07-35-034/output/model.tar.gz\",  # path to your trained SageMaker model\n",
    "   role=role,                                            # IAM role with permissions to create an endpoint\n",
    "   transformers_version=\"4.6\",                           # Transformers version used\n",
    "   pytorch_version=\"1.7\",                                # PyTorch version used\n",
    "   py_version='py36',                                    # Python version used\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "972c9f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.g4dn.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e404f4",
   "metadata": {},
   "source": [
    "# endpoint调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0eddde43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor\n",
    "predictor=HuggingFacePredictor(endpoint_name='huggingface-pytorch-inference-2022-09-26-14-20-39-688')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84acf53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'location of use is in my shower'}]\n",
      "[{'generated_text': 'purchase intention is None'}]\n",
      "[{'generated_text': 'time of use is None'}]\n",
      "[{'generated_text': 'target consumer is None'}]\n",
      "1.1650428771972656\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s=time.time()\n",
    "# profix=['usage location : ', 'application : ', 'device : ', 'identity : ']\n",
    "profix=['location of use : ', 'purchase intention : ', 'time of use : ', 'target consumer : ']\n",
    "for i in profix:\n",
    "    out=predictor.predict({\n",
    "        'inputs': [i+\"it is in my shower and together with the teak stool , makes it feel like i 'm at a spa whenever i shower .\"],\n",
    "        \"parameters\": {\"max_length\": 256},\n",
    "    })\n",
    "    print(out)\n",
    "e=time.time()\n",
    "print(e-s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ab413f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"application : it is in my shower and together with the teak stool , makes it feel like i 'm at a spa whenever i shower .\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./data/shulex_dev.csv')\n",
    "data.loc[121,'ref']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b15c8f3",
   "metadata": {},
   "source": [
    "# 本地训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331e5226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with `--source_prefix 'summarize: ' `\n",
      "09/07/2022 07:36:03 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "09/07/2022 07:36:03 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='models', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Sep07_07-36-03_ip-172-16-69-247.us-west-2.compute.internal', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.EPOCH: 'epoch'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='models', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
      "09/07/2022 07:36:03 - WARNING - datasets.builder -   Using custom data configuration default-8d2b088c53ab6cb7\n",
      "Downloading and preparing dataset csv/default to /home/ec2-user/.cache/huggingface/datasets/csv/default-8d2b088c53ab6cb7/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n",
      "Downloading data files: 100%|███████████████████| 2/2 [00:00<00:00, 8430.76it/s]\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1411.75it/s]\n",
      "Dataset csv downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/csv/default-8d2b088c53ab6cb7/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 571.98it/s]\n",
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /home/ec2-user/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /home/ec2-user/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
      "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 47/47 [00:02<00:00, 18.13ba/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 20.09ba/s]\n",
      "***** Running training *****\n",
      "  Num examples = 46800\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11700\n",
      "  0%|                                                 | 0/11700 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"scripts/run_paraphrase.py\", line 606, in <module>\n",
      "    main()\n",
      "  File \"scripts/run_paraphrase.py\", line 530, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/transformers/trainer.py\", line 1318, in train\n",
      "    self.optimizer.step()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/transformers/optimization.py\", line 347, in step\n",
      "    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.78 GiB total capacity; 819.00 MiB already allocated; 15.94 MiB free; 844.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  0%|                                                 | 0/11700 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!python scripts/run_paraphrase.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_file data/shulex_train.csv \\\n",
    "    --validation_file data/shulex_dev.csv \\\n",
    "    --output_dir /tmp/tst-summarization \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_strategy 'epoch' \\\n",
    "    --reference_column 'ref' \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --hypothesis_column 'hyp' \\\n",
    "    --max_source_length 128 \\\n",
    "    --output_dir models \\\n",
    "    --max_target_length 128 \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a27bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: datasets>=1.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from -r scripts/requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from -r scripts/requirements.txt (line 2)) (0.1.97)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from -r scripts/requirements.txt (line 3)) (3.19.1)\n",
      "Requirement already satisfied: rouge-score in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from -r scripts/requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from -r scripts/requirements.txt (line 5)) (3.7)\n",
      "Requirement already satisfied: py7zr in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from -r scripts/requirements.txt (line 6)) (0.20.0)\n",
      "Collecting transformers==4.6.1\n",
      "  Using cached transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: torch>=1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from -r scripts/requirements.txt (line 8)) (1.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers==4.6.1->-r scripts/requirements.txt (line 7)) (1.20.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers==4.6.1->-r scripts/requirements.txt (line 7)) (2.26.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers==4.6.1->-r scripts/requirements.txt (line 7)) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers==4.6.1->-r scripts/requirements.txt (line 7)) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers==4.6.1->-r scripts/requirements.txt (line 7)) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers==4.6.1->-r scripts/requirements.txt (line 7)) (4.62.3)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
      "  Using cached huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets>=1.1.3->-r scripts/requirements.txt (line 1)) (1.3.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets>=1.1.3->-r scripts/requirements.txt (line 1)) (7.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets>=1.1.3->-r scripts/requirements.txt (line 1)) (3.8.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets>=1.1.3->-r scripts/requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets>=1.1.3->-r scripts/requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets>=1.1.3->-r scripts/requirements.txt (line 1)) (2021.11.1)\n",
      "Collecting datasets>=1.1.3\n",
      "  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.3/362.3 KB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-2.3.1-py3-none-any.whl (362 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.3/362.3 KB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-2.3.0-py3-none-any.whl (361 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.9/361.9 KB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 KB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets>=1.1.3->-r scripts/requirements.txt (line 1)) (0.3.4)\n",
      "  Downloading datasets-2.2.1-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.2/342.2 KB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-2.2.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.3/342.3 KB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 KB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 KB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 KB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 KB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.18.2-py3-none-any.whl (312 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.0/312.0 KB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.18.1-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.5/311.5 KB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.18.0-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.3/311.3 KB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.3/306.3 KB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.3/298.3 KB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.16.0-py3-none-any.whl (298 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.3/298.3 KB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.6/290.6 KB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.15.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.7/290.7 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 KB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.1/287.1 KB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.13.2-py3-none-any.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.1/287.1 KB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.13.1-py3-none-any.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.1/287.1 KB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.13.0-py3-none-any.whl (285 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.4/285.4 KB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.0/270.0 KB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.12.0-py3-none-any.whl (269 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.6/269.6 KB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.9/264.9 KB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets>=1.1.3->-r scripts/requirements.txt (line 1)) (0.70.12.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from rouge-score->-r scripts/requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from rouge-score->-r scripts/requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nltk->-r scripts/requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nltk->-r scripts/requirements.txt (line 5)) (8.0.3)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from py7zr->-r scripts/requirements.txt (line 6)) (0.2.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from py7zr->-r scripts/requirements.txt (line 6)) (5.8.0)\n",
      "Requirement already satisfied: pyppmd<0.19.0,>=0.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from py7zr->-r scripts/requirements.txt (line 6)) (0.18.3)\n",
      "Requirement already satisfied: texttable in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from py7zr->-r scripts/requirements.txt (line 6)) (1.6.4)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from py7zr->-r scripts/requirements.txt (line 6)) (0.15.3)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from py7zr->-r scripts/requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: inflate64>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from py7zr->-r scripts/requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from py7zr->-r scripts/requirements.txt (line 6)) (3.15.0)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from py7zr->-r scripts/requirements.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from torch>=1.3->-r scripts/requirements.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r scripts/requirements.txt (line 7)) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r scripts/requirements.txt (line 7)) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r scripts/requirements.txt (line 7)) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r scripts/requirements.txt (line 7)) (1.26.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from packaging->transformers==4.6.1->-r scripts/requirements.txt (line 7)) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->datasets>=1.1.3->-r scripts/requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->datasets>=1.1.3->-r scripts/requirements.txt (line 1)) (2021.3)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=abe1336268bc20120f6329ff4b1695738ea6bfa5d6569d6e4883c83f3cb03197\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers, datasets\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.9.1\n",
      "    Uninstalling huggingface-hub-0.9.1:\n",
      "      Successfully uninstalled huggingface-hub-0.9.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.4.0\n",
      "    Uninstalling datasets-2.4.0:\n",
      "      Successfully uninstalled datasets-2.4.0\n",
      "Successfully installed datasets-1.11.0 huggingface-hub-0.0.8 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.6.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r scripts/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da90e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
